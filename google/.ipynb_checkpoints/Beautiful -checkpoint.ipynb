{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Datetime', 'Search term', 'Google URL ', 'Ad URL Website', 'Vendor',\n",
       "       'Position Num', 'Position', 'Result is consistent', 'Page number',\n",
       "       'Type of result', 'Comments', 'Ad value', 'Content Link'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sheet = pd.read_excel('Data.xlsx', sheetname='Abhishek')\n",
    "Sheet['Ad value'] = \"\"\n",
    "Sheet.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IP Address Bouncer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOWNLOADER_MIDDLEWARES = {\n",
    "        'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 90,\n",
    "        'tutorial.randomproxy.RandomProxy': 100,\n",
    "        'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 110,\n",
    "        'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware' : None,\n",
    "        'tutorial.spiders.rotate_useragent.RotateUserAgentMiddleware' :400,\n",
    "\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    #the default user_agent_list composes chrome,I E,firefox,Mozilla,opera,netscape\n",
    "    #for more user agent strings,you can find it in http://www.useragentstring.com/pages/useragentstring.php\n",
    "\n",
    "user_agent_list = [\\\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1\"\\\n",
    "        \"Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11\",\\\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6\",\\\n",
    "        \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6\",\\\n",
    "        \"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1\",\\\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5\",\\\n",
    "        \"Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5\",\\\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\",\\\n",
    "        \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\",\\\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\",\\\n",
    "        \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3\",\\\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3\",\\\n",
    "        \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\",\\\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\",\\\n",
    "        \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\",\\\n",
    "        \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3\",\\\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24\",\\\n",
    "        \"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24\"\n",
    "       ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def process_request():\n",
    "    ua = random.choice(user_agent_list)\n",
    "    return ua"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Ad Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 UN32EH4003 Samsung\n",
      "http://www.google.com/search?q=UN32EH4003+Samsung&num=100&hl=en&start=0\n",
      "UN32EH4003 Samsung - Google Search\n",
      "http://www.google.com/search?q=UN32EH4003+Samsung&num=100&hl=en&start=0\n",
      "SearchTerm UN32EH4003 Samsung\n",
      "hello\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-1abfe8c851cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mdo_all_keywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-1abfe8c851cd>\u001b[0m in \u001b[0;36mdo_all_keywords\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0msoup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_google_search_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keywordPhrase'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mscrape_ads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'phraseID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keywordPhrase'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-1abfe8c851cd>\u001b[0m in \u001b[0;36mscrape_ads\u001b[0;34m(soup, phraseID, SearchTerm, address)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hello\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult_block\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0msite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'plantl'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mline1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'span'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'rhsl4'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mPrice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'span'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'_kh'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import bs4 as bs\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "SearchTerm = \"\"\n",
    "\n",
    "def get_google_search_results(keyword):\n",
    "    address = \"http://www.google.com/search?q=%s&num=100&hl=en&start=0\" % (urllib.parse.quote_plus(keyword))\n",
    "    user = process_request()\n",
    "    request = urllib.request.Request(address, None, {'User-Agent':user})#'Mosilla/5.0 (Macintosh; Intel Mac OS X 10_7_4) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11'\n",
    "    urlfile = urllib.request.urlopen(request)\n",
    "    page = urlfile.read()\n",
    "    print(address)\n",
    "    soup = bs.BeautifulSoup(page,'lxml')\n",
    "    print(soup.title.string)\n",
    "    SearchTerm = keyword\n",
    "    return soup,address\n",
    "\n",
    "def add_sheet(link):\n",
    "    print(\"hi\")\n",
    "\n",
    "def scrape_ads(soup, phraseID, SearchTerm, address):\n",
    "    \"\"\"Scrape the text as HTML, find and parse out all the ads and store them in a database\n",
    "    \"\"\"    \n",
    "\n",
    "    print(\"SearchTerm\",SearchTerm)\n",
    "    #get the ads on the right hand side of the page\n",
    "    prices = soup.findAll('div', attrs={'class':'mnr-c pla-unit'})\n",
    "    Mainads = soup.findAll('li', attrs={'class':'ads-ad'})\n",
    "    Bottom = soup.findAll('div', attrs={'id':'bottomads'})\n",
    "    position = 0\n",
    "    \n",
    "    result_block = soup.findAll('div', attrs={'class':'_Dw'})\n",
    "    print(\"hello\")\n",
    "    for ad in result_block:\n",
    "        site = ad.find('a',attrs={'class':'plantl'})['href']\n",
    "        line1 = ad.find('span', attrs={'class': 'rhsl4'}).text\n",
    "        Price = ad.find('span', attrs={'class': '_kh'}).text\n",
    "        print(Price)\n",
    "        position += 1\n",
    "        arow2 = [datetime.datetime.now(), SearchTerm, address, site, 'NA',position, 'RHS', 'NA', '1','Sponsored Ad', 'NA',line1]\n",
    "        Sheet.loc[len(Sheet)] = arow2\n",
    "\n",
    "        \n",
    "    position = 0\n",
    "    #Main Ads    \n",
    "    for ad in Mainads:\n",
    "        position += 1\n",
    "        #display url\n",
    "        parts = ad.find('cite').findAll(text=True)\n",
    "        site = ''.join([word.strip() for word in parts]).strip()\n",
    "        ad.find('cite').replaceWith(\"\")\n",
    "        print(site)\n",
    " \n",
    "        #the header line\n",
    "        parts = ad.find('a').findAll(text=True)\n",
    "        title = ' '.join([word.strip() for word in parts]).strip()\n",
    " \n",
    "        #the destination URL\n",
    "        href = ad.find('a')['href']\n",
    "        start = href.find('&q=')\n",
    "        if start != -1 :\n",
    "            dest = href[start+3:]\n",
    "        else :\n",
    "            dest = None\n",
    "            print ('error', href)\n",
    " \n",
    "        ad.find('a').replaceWith(\"\")\n",
    "    \n",
    "        #body of ad\n",
    "        brs = ad.findAll('br')\n",
    "        for br in brs:\n",
    "            br.replaceWith(\"%BR%\")\n",
    "        parts = ad.findAll(text=True)\n",
    "        body = ' '.join([word.strip() for word in parts]).strip()\n",
    "        line1 = body.split('%BR%')[0].strip()\n",
    "        #line2 = body.split('%BR%')[1].strip()\n",
    "        #['Datetime', 'Search term', 'Google URL ', 'Ad URL Website', 'Vendor','Position Num', 'Position', 'Result is consistent', 'Page number','Type of result', 'Comments', 'Ad value']\n",
    "        arow2 = [datetime.datetime.now(), SearchTerm, address, site, 'NA',position, 'Main Ad', 'NA', '1','Sponsored Ad', 'NA',line1]\n",
    "        Sheet.loc[len(Sheet)] = arow2\n",
    "\n",
    "\n",
    "\n",
    "def get_all_keywords():\n",
    "    #Read the file\n",
    "    df = pd.read_excel('productsSample.xlsx', sheetname='Table1')\n",
    "    ## Get the Search String\n",
    "    queryString = df['model']+\" \"+df['brand'] ## if model number not present  -> product name \n",
    "    queryString = queryString.to_frame()\n",
    "    queryString = queryString.rename(columns= {0: 'keywordPhrase'})\n",
    "    queryString.insert(0, 'phraseID', range(1, 1 + len(queryString)))\n",
    "    return queryString\n",
    "        \n",
    "def do_all_keywords():\n",
    "    queryString = get_all_keywords()\n",
    "    resultnew = queryString.head(n=10)\n",
    "    for index, row in resultnew.iterrows():\n",
    "        print (row['phraseID'], row['keywordPhrase'])\n",
    "        soup, address = get_google_search_results(row['keywordPhrase'])\n",
    "        print(address)\n",
    "        scrape_ads(soup, row['phraseID'],row['keywordPhrase'],address)\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == '__main__' :\n",
    "    do_all_keywords()        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Sheet.to_csv('../data/Ads.csv')\n",
    "#Sheet = Sheet.iloc[0:0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organic Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n",
      "write() argument must be str, not bytes\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-f3ed4dabb133>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;31m#print(data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "user = process_request()\n",
    "\n",
    "USER_AGENT = {'User-Agent':user}\n",
    "\n",
    "\n",
    "def fetch_results(search_term, number_results, language_code):\n",
    "    assert isinstance(search_term, str), 'Search term must be a string'\n",
    "    assert isinstance(number_results, int), 'Number of results must be an integer'\n",
    "    escaped_search_term = search_term.replace(' ', '+')\n",
    "\n",
    "    google_url = 'https://www.google.com/search?q={}&num={}&hl={}'.format(escaped_search_term, number_results, language_code)\n",
    "    response = requests.get(google_url, headers=USER_AGENT)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    return search_term, response.text, google_url\n",
    "\n",
    "\n",
    "def parse_results(html, keyword,google_url):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    found_results = []\n",
    "    rank = 1\n",
    "    result_block = soup.find_all('div', attrs={'class': 'g'})\n",
    "    for result in result_block:\n",
    "\n",
    "        link = result.find('a', href=True)\n",
    "        title = result.find('h3', attrs={'class': 'r'})\n",
    "        description = result.find('span', attrs={'class': 'st'})\n",
    "        if link and title:\n",
    "            link = link['href']\n",
    "            #print(link)\n",
    "            title = title.get_text()\n",
    "            description = description.get_text()\n",
    "            if link != '#':\n",
    "                found_results.append({'keyword': keyword, 'rank': rank, 'title': title, 'description': description})\n",
    "                rank += 1\n",
    "                #['Datetime', 'Search term', 'Google URL ', 'Ad URL Website', 'Vendor','Position Num', 'Position', 'Result is consistent', 'Page number','Type of result', 'Comments', 'Ad value']\n",
    "                response = requests.get(link)\n",
    "                webContent = response.text\n",
    "                print(type(webContent))\n",
    "                with open('link.html', 'w+') as f:\n",
    "                    f.write(webContent)\n",
    "                arow2 = [datetime.datetime.now(), keyword, google_url, link, 'NA',rank, 'organic search', 'NA', '1','organic search', 'NA',title]\n",
    "                Sheet.loc[len(Sheet)] = arow2\n",
    "    return found_results\n",
    "\n",
    "\n",
    "def scrape_google(search_term, number_results, language_code):\n",
    "    try:\n",
    "        keyword, html, google_url = fetch_results(search_term, number_results, language_code)\n",
    "        results = parse_results(html, keyword, google_url)\n",
    "        return results\n",
    "    except AssertionError:\n",
    "        raise Exception(\"Incorrect arguments parsed to function\")\n",
    "    except requests.HTTPError:\n",
    "        raise Exception(\"You appear to have been blocked by Google\")\n",
    "    except requests.RequestException:\n",
    "        raise Exception(\"Appears to be an issue with your connection\")\n",
    "        \n",
    "def get_all_keywords():\n",
    "    #Read the file\n",
    "    df = pd.read_excel('productsSample.xlsx', sheetname='Table1')\n",
    "    ## Get the Search String\n",
    "    queryString = df['model']+\" \"+df['brand'] ## if model number not present  -> product name \n",
    "    return queryString\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    keywords = get_all_keywords()#['iphone']\n",
    "    data = []\n",
    "    for keyword in keywords:\n",
    "        try:\n",
    "            results = scrape_google(keyword, 20, \"en\")\n",
    "            for result in results:\n",
    "                data.append(result)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        finally:\n",
    "            time.sleep(10)\n",
    "    #print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Sheet.to_csv('../data/Sheet.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
